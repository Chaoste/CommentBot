{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial on Keras with Gensim\n",
    "https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/\n",
    "\n",
    "- http://www.orbifold.net/default/2017/01/10/embedding-and-tokenizer-in-keras/\n",
    "- https://github.com/keras-team/keras/issues/853\n",
    "- http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/\n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "- https://stats.stackexchange.com/questions/320701/how-to-use-keras-pre-trained-embedding-layer\n",
    "- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'text_utils' from 'C:\\\\Users\\\\Thomas\\\\HPI\\\\Text Mining in Practice\\\\analysis\\\\text_utils.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "%matplotlib inline\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "import text_utils\n",
    "importlib.reload(text_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OUTPUT_DIR = './week-6-plots'\n",
    "ORIG_COMMENTS = '../data/guardian-all/sorted_comments-standardized-pol-all.csv'\n",
    "TOKENIZED_COMMENTS = '../data/embedding/sorted_comments-standardized-tokenized.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../data/guardian-all/sorted_comments-standardized-tokenized.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7b70c0bd3786>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_comments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTOKENIZED_COMMENTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\thomas\\python\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\python\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\python\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\python\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thomas\\python\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'../data/guardian-all/sorted_comments-standardized-tokenized.csv' does not exist"
     ]
    }
   ],
   "source": [
    "tokenized_comments = pd.read_csv(TOKENIZED_COMMENTS, ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4d5448e413f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_comments' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_comments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_articles, data_articles_pol, data_authors, data_comments_pol = utils.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_comments_pol['comment_text']\n",
    "y = pd.get_dummies(['pos' if x > 2 else 'neg' for x in data_comments_pol['upvotes']]).values  # 2 -> ? vs. ?\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = text_utils.load_embedding()\n",
    "word_vectors = ft_model.wv\n",
    "EMBEDDING_DIM = 50\n",
    "print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 665523/665523 [01:58<00:00, 5629.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup tokenizer\n",
    "# tokenizer = WordPunctTokenizer()\n",
    "# Use a counter for selecting the X most common words (therefore tokenize)\n",
    "# vocab = Counter()\n",
    "# comments = text_utils.process_comments(tokenizer, vocab, X, lower=True)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "index_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtranslater = dict((i, w) for w, i in index_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble the embedding_weights in one numpy array\n",
    "n_symbols = len(index_dict) + 1 # adding 1 to account for 0th index (for masking)\n",
    "embedding_weights = np.zeros((n_symbols, EMBEDDING_DIM))\n",
    "for word, index in index_dict.items():\n",
    "    if word in word_vectors:\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "\n",
    "# define inputs here\n",
    "# embedding_layer = Embedding(output_dim=vocab_dim, input_dim=n_symbols, weights=[embedding_weights], trainable=False)\n",
    "# embedded = embedding_layer(input_layer)\n",
    "# embedding_layer = model.wv.get_keras_embedding(train_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('president,', 0.708040177822113),\n",
       " ('presdident', 0.7070919871330261),\n",
       " ('presiden', 0.6998500227928162),\n",
       " ('president,a', 0.6993380784988403),\n",
       " ('foreiner', 0.697151243686676),\n",
       " ('Bundespresident', 0.6964511275291443),\n",
       " ('presidento', 0.6927176117897034),\n",
       " ('indident', 0.6924108266830444),\n",
       " ('Bundespraesident', 0.6900625228881836),\n",
       " ('president,but', 0.6889559626579285)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['president', 'german'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9264782071113586),\n",
       " ('regnant', 0.901670515537262),\n",
       " ('king/queen', 0.8998395800590515),\n",
       " ('monarhy', 0.8881024718284607),\n",
       " ('royal', 0.8818458914756775),\n",
       " ('regent', 0.8790533542633057),\n",
       " ('virgina', 0.8765912652015686),\n",
       " ('monarch', 0.8760467171669006),\n",
       " ('empress', 0.8739269375801086),\n",
       " ('prince', 0.8708903789520264)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad/Cut tokenized comments to a certain length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data tensor: (532418, 200)\n",
      "Shape of test_data tensor: (133105, 200)\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = len(word_vectors.vocab)\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "train_size = len(X_train)\n",
    "most_common = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "# word_index tells ous the\n",
    "X_train, X_test, word_index = text_utils.pad_or_cut_tokenized_comments(\n",
    "    most_common, comments, train_size, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://codekansas.github.io/blog/2016/gensim.html\n",
    "WV_DIM = 50\n",
    "nb_words = min(MAX_NB_WORDS, len(word_vectors.vocab))\n",
    "wv_matrix = word_vectors.syn0 #  text_utils.get_weights_matrix(word_index, word_vectors, nb_words, WV_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 64\n",
    "batch_size = 16\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,\n",
    "                    WV_DIM,\n",
    "                    mask_zero=False,\n",
    "                    weights=[wv_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "# model.add(Embedding(vocabulary, embed_dim, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "# model.add(LSTM(lstm_out, return_sequences=True, recurrent_dropout=0.3, dropout=0.3))\n",
    "model.add(LSTM(lstm_out, recurrent_dropout=0, dropout=0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 50)           64307550  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 64,337,120\n",
      "Trainable params: 29,570\n",
      "Non-trainable params: 64,307,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      " - 379s - loss: 0.6946 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.5014\n",
      "Epoch 2/20\n",
      " - 171s - loss: 0.6933 - acc: 0.5081 - val_loss: 0.6941 - val_acc: 0.4850\n",
      "Epoch 3/20\n",
      " - 184s - loss: 0.6930 - acc: 0.5076 - val_loss: 0.6927 - val_acc: 0.5166\n",
      "Epoch 4/20\n",
      " - 188s - loss: 0.6928 - acc: 0.5095 - val_loss: 0.6943 - val_acc: 0.4840\n",
      "Epoch 5/20\n",
      " - 191s - loss: 0.6925 - acc: 0.5148 - val_loss: 0.6935 - val_acc: 0.5096\n",
      "Epoch 6/20\n",
      " - 193s - loss: 0.6921 - acc: 0.5148 - val_loss: 0.6932 - val_acc: 0.5148\n",
      "Epoch 7/20\n",
      " - 184s - loss: 0.6916 - acc: 0.5204 - val_loss: 0.6948 - val_acc: 0.5086\n",
      "Epoch 8/20\n",
      " - 168s - loss: 0.6910 - acc: 0.5231 - val_loss: 0.6960 - val_acc: 0.5072\n",
      "Epoch 9/20\n",
      " - 148s - loss: 0.6903 - acc: 0.5282 - val_loss: 0.6961 - val_acc: 0.5076\n",
      "Epoch 10/20\n",
      " - 144s - loss: 0.6887 - acc: 0.5340 - val_loss: 0.6982 - val_acc: 0.5076\n",
      "Epoch 11/20\n",
      " - 145s - loss: 0.6874 - acc: 0.5364 - val_loss: 0.6992 - val_acc: 0.5066\n",
      "Epoch 12/20\n",
      " - 140s - loss: 0.6866 - acc: 0.5412 - val_loss: 0.7003 - val_acc: 0.5050\n",
      "Epoch 13/20\n",
      " - 132s - loss: 0.6838 - acc: 0.5500 - val_loss: 0.7019 - val_acc: 0.5016\n",
      "Epoch 14/20\n",
      " - 128s - loss: 0.6825 - acc: 0.5559 - val_loss: 0.7039 - val_acc: 0.5090\n",
      "Epoch 15/20\n",
      " - 127s - loss: 0.6807 - acc: 0.5552 - val_loss: 0.7031 - val_acc: 0.4988\n",
      "Epoch 16/20\n",
      " - 127s - loss: 0.6769 - acc: 0.5652 - val_loss: 0.7121 - val_acc: 0.4936\n",
      "Epoch 17/20\n",
      " - 127s - loss: 0.6749 - acc: 0.5699 - val_loss: 0.7168 - val_acc: 0.4894\n",
      "Epoch 18/20\n",
      " - 127s - loss: 0.6716 - acc: 0.5763 - val_loss: 0.7172 - val_acc: 0.4952\n",
      "Epoch 19/20\n",
      " - 127s - loss: 0.6696 - acc: 0.5798 - val_loss: 0.7182 - val_acc: 0.4890\n",
      "Epoch 20/20\n",
      " - 127s - loss: 0.6660 - acc: 0.5869 - val_loss: 0.7195 - val_acc: 0.4890\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Here we train the Network.\n",
    "try:\n",
    "    history = model.fit([X_train[:50000]], y_train[:50000], validation_split=0.1,\n",
    "                        batch_size = batch_size, epochs = epochs,\n",
    "                        verbose = 2, shuffle=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Fitting stopped manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Unnamed: 0                                                      159895\n",
      "article_id                                                        1731\n",
      "author_id                                                        10860\n",
      "comment_id                                                    61083280\n",
      "comment_text         Actually it has - it was last night, I was the...\n",
      "timestamp                                         2015-10-09T08:30:36Z\n",
      "parent_comment_id                                          6.10705e+07\n",
      "upvotes                                                              2\n",
      "Name: 0, dtype: object\n",
      "\n",
      "After preprocessing (& backtranslating):\n",
      "actually it has it was last night i was there unless he ' s going to give another speech at the university of sheffield in which he says these things\n"
     ]
    }
   ],
   "source": [
    "def inspect_preprocessed_comment(data_comments_pol, X_train, backtranslater, idx):\n",
    "    print('Original:')\n",
    "    print(data_comments_pol.iloc[idx])\n",
    "    print('\\nAfter preprocessing (& backtranslating):')\n",
    "    print(' '.join([backtranslater[x] for x in X_train[idx] if x in backtranslater]))\n",
    "\n",
    "inspect_preprocessed_comment(data_comments_pol, X_train, backtranslater, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring score and accuracy on test set\n",
    "\n",
    "score, acc = model.evaluate([X:test], y_test, verbose = 2,\n",
    "                            batch_size = batch_size)\n",
    "print(\"Logloss score: %.2f\" % (score))\n",
    "print(\"Test set Accuracy: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_comments_pol['comment_text'].str.split().len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
