{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial on Keras with Gensim\n",
    "https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import describe\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "from string import punctuation, ascii_lowercase\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OUTPUT_DIR = './week-6-plots'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_articles, data_articles_pol, data_authors, data_comments_pol = utils.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_comments_pol['comment_text']\n",
    "y = data_comments_pol['upvotes']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = ['pos' if x > 2 else 'neg' for x in y_test]  # 2 -> ? vs. ?\n",
    "y_test = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = ['pos' if x > 2 else 'neg' for x in y_train]  # 2 -> ? vs. ?\n",
    "y_train = pd.get_dummies(y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have no NaN words but it could be useful\n",
    "list_sentences_train = list(X_train.fillna(\"NAN_WORD\").values)\n",
    "list_sentences_test = list(X_test.fillna(\"NAN_WORD\").values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 665523/665523 [05:15<00:00, 2108.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Replace urls\n",
    "re_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\\n",
    "                    .([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\n",
    "                    re.MULTILINE|re.UNICODE)\n",
    "# Replace ips\n",
    "re_ip = re.compile(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\")\n",
    "\n",
    "\n",
    "# Setup tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# Use a counter for selecting the X most common words (therefore tokenize)\n",
    "vocab = Counter()\n",
    "\n",
    "def text_to_wordlist(text, lower=False):\n",
    "    # Looks like all URLs are removed\n",
    "    text = re_url.sub(\"URL\", text)\n",
    "    \n",
    "    # But there some IPs we'd like to replace\n",
    "    text = re_ip.sub(\"IPADDRESS\", text)\n",
    "    \n",
    "    # Tokenize\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # optional: lower case\n",
    "    if lower:\n",
    "        text = [t.lower() for t in text]\n",
    "    \n",
    "    # Return a list of words\n",
    "    vocab.update(text)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return text\n",
    "\n",
    "def process_comments(list_sentences, lower=False):\n",
    "    comments = []\n",
    "    for text in tqdm(list_sentences):\n",
    "        txt = text_to_wordlist(text, lower=lower)\n",
    "        comments.append(txt)\n",
    "    return comments\n",
    "\n",
    "comments = process_comments(list_sentences_train + list_sentences_test, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n"
     ]
    }
   ],
   "source": [
    "ft_model = utils.load_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = ft_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors: 1286151\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('president,', 0.708040177822113),\n",
       " ('presdident', 0.7070919871330261),\n",
       " ('presiden', 0.6998500227928162),\n",
       " ('president,a', 0.6993380784988403),\n",
       " ('foreiner', 0.697151243686676),\n",
       " ('Bundespresident', 0.6964511275291443),\n",
       " ('presidento', 0.6927176117897034),\n",
       " ('indident', 0.6924108266830444),\n",
       " ('Bundespraesident', 0.6900625228881836),\n",
       " ('president,but', 0.6889559626579285)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['president', 'german'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9264782071113586),\n",
       " ('regnant', 0.901670515537262),\n",
       " ('king/queen', 0.8998395800590515),\n",
       " ('monarhy', 0.8881024718284607),\n",
       " ('royal', 0.8818458914756775),\n",
       " ('regent', 0.8790533542633057),\n",
       " ('virgina', 0.8765912652015686),\n",
       " ('monarch', 0.8760467171669006),\n",
       " ('empress', 0.8739269375801086),\n",
       " ('prince', 0.8708903789520264)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad/Cut tokenized comments to a certain length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = len(word_vectors.vocab)\n",
    "MAX_SEQUENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (532418, 200)\n",
      "Shape of label tensor: (665523,)\n",
      "Shape of test_data tensor: (133105, 200)\n"
     ]
    }
   ],
   "source": [
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "sequences = [[word_index.get(t, 0) for t in comment]\n",
    "             for comment in comments[:len(list_sentences_train)]]\n",
    "test_sequences = [[word_index.get(t, 0)  for t in comment] \n",
    "                  for comment in comments[len(list_sentences_train):]]\n",
    "\n",
    "# pad\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding=\"pre\", truncating=\"post\")\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y_label.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                          truncating=\"post\")\n",
    "print('Shape of test_data tensor:', test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "WV_DIM = 100\n",
    "nb_words = min(MAX_NB_WORDS, len(word_vectors.vocab))\n",
    "# we initialize the matrix with random numbers\n",
    "wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 64\n",
    "batch_size = 16\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,\n",
    "                    WV_DIM,\n",
    "                    mask_zero=False,\n",
    "                    weights=[wv_matrix],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "# model.add(Embedding(vocabulary, embed_dim, input_length = X.shape[1]))\n",
    "# model.add(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "# model.add(LSTM(lstm_out, return_sequences=True, recurrent_dropout=0.3, dropout=0.3))\n",
    "model.add(LSTM(lstm_out, recurrent_dropout=0, dropout=0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 100)          128615100 \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 128,657,470\n",
      "Trainable params: 42,370\n",
      "Non-trainable params: 128,615,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      " - 208s - loss: 0.6897 - acc: 0.5296 - val_loss: 0.6865 - val_acc: 0.5410\n",
      "Epoch 2/20\n",
      " - 212s - loss: 0.6853 - acc: 0.5505 - val_loss: 0.6953 - val_acc: 0.5052\n",
      "Epoch 3/20\n",
      " - 212s - loss: 0.6863 - acc: 0.5458 - val_loss: 0.6842 - val_acc: 0.5536\n",
      "Epoch 4/20\n",
      " - 165s - loss: 0.6843 - acc: 0.5546 - val_loss: 0.6857 - val_acc: 0.5452\n",
      "Epoch 5/20\n",
      " - 138s - loss: 0.6829 - acc: 0.5574 - val_loss: 0.6847 - val_acc: 0.5610\n",
      "Epoch 6/20\n",
      " - 136s - loss: 0.6820 - acc: 0.5595 - val_loss: 0.6861 - val_acc: 0.5490\n",
      "Epoch 7/20\n",
      " - 149s - loss: 0.6805 - acc: 0.5660 - val_loss: 0.6860 - val_acc: 0.5540\n",
      "Epoch 8/20\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Here we train the Network.\n",
    "try:\n",
    "    history = model.fit([data[:50000]], y_train[:50000], validation_split=0.1,\n",
    "                        batch_size = batch_size, epochs = epochs,\n",
    "                        verbose = 2, shuffle=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Fitting stopped manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring score and accuracy on test set\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, verbose = 2,\n",
    "                            batch_size = batch_size)\n",
    "print(\"Logloss score: %.2f\" % (score))\n",
    "print(\"Test set Accuracy: %.2f\" % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
