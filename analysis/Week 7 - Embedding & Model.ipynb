{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial on Keras with Gensim\n",
    "https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/\n",
    "\n",
    "- http://www.orbifold.net/default/2017/01/10/embedding-and-tokenizer-in-keras/\n",
    "- https://github.com/keras-team/keras/issues/853\n",
    "- http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/\n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "- https://stats.stackexchange.com/questions/320701/how-to-use-keras-pre-trained-embedding-layer\n",
    "- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "- https://codekansas.github.io/blog/2016/gensim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\python\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\thomas\\hpi\\textmi~1\\venv\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "%matplotlib inline\n",
    "\n",
    "import utils\n",
    "import text_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OUTPUT_DIR = './week-7-plots'\n",
    "\n",
    "SRC_ENHANCED_COMMENTS = '../data/pol/comments-root-all-pol-enhanced.csv'\n",
    "\n",
    "SRC_GENSIM_EMBEDDING = '../data/embedding/gensim-guardian-comments-50-tokenized.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Number of word vectors: 1286151\n"
     ]
    }
   ],
   "source": [
    "word_vectors = text_utils.load_embedding()\n",
    "index2word = word_vectors.index2word  # Map index to word\n",
    "word2index = dict((x, y) for y,x in enumerate(word_vectors.index2word))  # Map word to index\n",
    "\n",
    "print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9264782071113586),\n",
       " ('regnant', 0.901670515537262),\n",
       " ('king/queen', 0.8998395800590515),\n",
       " ('monarhy', 0.8881024718284607),\n",
       " ('royal', 0.8818458914756775),\n",
       " ('regent', 0.8790533542633057),\n",
       " ('virgina', 0.8765912652015686),\n",
       " ('monarch', 0.8760467171669006),\n",
       " ('empress', 0.8739269375801086),\n",
       " ('prince', 0.8708903789520264)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "comments = pd.read_csv(SRC_ENHANCED_COMMENTS)\n",
    "X = comments['comment_text']\n",
    "y = to_categorical(comments['bin'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Pad/Cut tokenized comments to a certain length\n",
    "X_train, X_test = text_utils.pad_or_cut_tokenized_comments(X_train, X_test, word2index, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_vectors.vocab),\n",
    "                    EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    weights=[word_vectors.syn0],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "# model.add(LSTM(64, return_sequences=True, recurrent_dropout=0.3, dropout=0.3))\n",
    "model.add(LSTM(64, recurrent_dropout=0, dropout=0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 50)           64307550  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 64,337,120\n",
      "Trainable params: 29,570\n",
      "Non-trainable params: 64,307,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57600 samples, validate on 6400 samples\n",
      "Epoch 1/20\n",
      " - 279s - loss: 0.6924 - acc: 0.5221 - val_loss: 0.6900 - val_acc: 0.5331\n",
      "Epoch 2/20\n",
      " - 166s - loss: 0.6906 - acc: 0.5287 - val_loss: 0.6901 - val_acc: 0.5369\n",
      "Epoch 3/20\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Here we train the Network.\n",
    "try:\n",
    "    history = model.fit([X_train], y_train, validation_split=0.1,\n",
    "                        batch_size = batch_size, epochs = epochs,\n",
    "                        verbose = 2, shuffle=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Fitting stopped manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_utils.inspect_preprocessed_comment(comments, X_train, index2word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring score and accuracy on test set\n",
    "\n",
    "score, acc = model.evaluate([X:test], y_test, verbose = 2,\n",
    "                            batch_size = batch_size)\n",
    "print(\"Logloss score: %.2f\" % (score))\n",
    "print(\"Test set Accuracy: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comments['comment_text'].str.split().len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
